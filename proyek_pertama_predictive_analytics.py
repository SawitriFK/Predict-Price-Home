# -*- coding: utf-8 -*-
"""Proyek Pertama : Predictive Analytics.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_9QCE-xMMBamyfUyRlETEcOUiusi6Uo_

# BUSINESS UNDERSTANDING

---

Memprediksi harga rumah secara akurat bisa menjadi tugas yang menakutkan. Pembeli hanya tidak peduli dengan ukuran (kaki persegi) rumah dan ada berbagai faktor lain yang memainkan peran kunci untuk menentukan harga rumah/properti.

# DATA UNDERSTANDING

---

## Data loading
"""

# Commented out IPython magic to ensure Python compatibility.
# LIBRARY

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns

# IDENTIFY DATA

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))
  
# Then move kaggle.json into the folder where the API expects to find it.
!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json

# DOWNLOAD DATA

!kaggle datasets download -d saipavansaketh/pune-house-data

# UNZIP DATA

from zipfile import ZipFile,os
file_name = "/content/pune-house-data.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall('/content/temp/')
  print('Done')

# LOAD THE DATASET

base_dir = '/content/temp/Delhi house data.csv'
df = pd.read_csv(base_dir)
df

"""## Exploratory Data Analysis

#### Deskripsi Variabel
"""

df.info()

df.describe()

"""#### Menangani Missing Value dan Outliers"""

df.isnull().sum()

"""Diasumsikan bahwa :
1. Fushising, Type, dan Per_Sqft harus terdefinisi
2. Terdapat kemungkinan bahwa TIDAK memiliki bathroom dan, Parking
3. Kolom Locality/alamat tidak diperlukan karena bukan kategori
"""

# DROP Nan and Column

df = df.dropna(subset=['Furnishing', 'Type', 'Per_Sqft']) # Menghapus value yang NaN sesuai asumsi (1)
df[['Bathroom', 'Parking']] = df[['Bathroom', 'Parking']].fillna(0) #Mengubah nilai Nan menjadi 0 sesuai asumsi (2)
del df['Locality'] # Menghapus kolom sesuai asumsi (3)

# df = df.dropna() # Delete NaN
df.shape

# VISUALISAZI OUTLIERS

fig, axes = plt.subplots(2, 3, figsize = (20,8))
 
fig.suptitle('Outliers - 2 x 3 axes Box plot with data')
sns.boxplot(ax=axes[0][0], data=df, x='Area')
sns.boxplot(ax=axes[0][1], data=df, x='BHK')
sns.boxplot(ax=axes[0][2], data=df, x='Bathroom')
sns.boxplot(ax=axes[1][0], data=df, x='Parking')
sns.boxplot(ax=axes[1][1], data=df, x='Per_Sqft')
axes[1][2].remove()

"""Terlihat bahwa masih banyak titik-titik yang menjauhi bar yang berarti masih banyak nilai-nilai yang didefinisikan jauh dari rata-rata nilai seharusnya"""

# DROP OUTLIERS

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR=Q3-Q1
df = df[~((df<(Q1-1.5*IQR))|(df>(Q3+1.5*IQR))).any(axis=1)]
 
# Cek ukuran dataset setelah drop outliers
df.shape

"""#### Univariate Analysis"""

numerical_features = ['Area', 'BHK', 'Bathroom', 'Parking', 'Price', 'Per_Sqft']
categorical_features = ['Furnishing', 'Status', 'Transaction', 'Type']

feature = categorical_features[0]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df0 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df0)
count.plot(kind='bar', title=feature, figsize=(5,2));

"""Pada diagram diatas terlihat bahwa properti yang terdata masih banyak menggunakan tipe Semi-Furnishing dibandingkan tipe yang lainnya."""

feature = categorical_features[1]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df1 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df1)
count.plot(kind='bar', title=feature, figsize=(5,2));

"""Pada diagram diatas terlihat bahwa properti yang terdata masih banyak menggunakan berstatus 'siap untuk pindah' dibandingkan 'selalu-siap'."""

feature = categorical_features[2]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df2 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df2)
count.plot(kind='bar', title=feature, figsize=(5,2));

"""Pada diagram diatas terlihat bahwa properti yang terdata masih banyak menggunakan transaksi dari properti yang dijual kembali dibandingkan pproperti yang masih baru."""

feature = categorical_features[3]
count = df[feature].value_counts()
percent = 100*df[feature].value_counts(normalize=True)
df3 = pd.DataFrame({'jumlah sampel':count, 'persentase':percent.round(1)})
print(df3)
count.plot(kind='bar', title=feature, figsize=(5,2));

"""Pada diagram diatas terlihat bahwa properti yang terdata masih banyak menggunakan tipe properti bangunan dibandingkan apartemen"""

df.hist(bins=50, figsize=(15,10))
plt.show()

"""#### Multivariate Analysis"""

at_features = df.select_dtypes(include='object').columns.to_list()
 
for col in categorical_features:
  sns.catplot(x=col, y="Price", kind="bar", dodge=False, height = 3, aspect = 3,  data=df, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))

sns.pairplot(df, diag_kind = 'kde')

plt.figure(figsize=(10, 8))
correlation_matrix = df.corr().round(2)

sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)

"""Pada fungsi target 'Price', nilai korelasi terlemah jatuh pada 'Parking'. Sehingga data parking tidak diperlukan dalam melakukan prediksi"""

df.drop(['Parking'], inplace=True, axis=1)
df.head()

"""# DATA PREPARATION

---

## Encoding Fitur Kategori
"""

from sklearn.preprocessing import  OneHotEncoder
df = pd.concat([df, pd.get_dummies(df['Furnishing'], prefix='Furnishing')],axis=1)
df = pd.concat([df, pd.get_dummies(df['Status'], prefix='Status')],axis=1)
df = pd.concat([df, pd.get_dummies(df['Transaction'], prefix='Transaction')],axis=1)
df = pd.concat([df, pd.get_dummies(df['Type'], prefix='Type')],axis=1)
df.drop(['Furnishing', 'Status', 'Transaction', 'Type'], axis=1, inplace=True)
df.head()

"""## Reduksi Dimensi dengan PCA"""

sns.pairplot(df[['Area','Bathroom', 'BHK']], plot_kws={"s": 3});

from sklearn.decomposition import PCA

pca = PCA(n_components=1, random_state=123)
pca.fit(df[['Area','Bathroom', 'BHK']])
df['dimension'] = pca.transform(df.loc[:, ('Area','Bathroom', 'BHK')]).flatten()
df.drop(['Area','Bathroom', 'BHK'], axis=1, inplace=True)

df.head()

"""## Train-Test-Split"""

from sklearn.model_selection import train_test_split
 
X = df.drop(["Price"],axis =1)
y = df["Price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 123)

print(f'Total # of sample in whole dataset: {len(X)}')
print(f'Total # of sample in train dataset: {len(X_train)}')
print(f'Total # of sample in test dataset: {len(X_test)}')

"""## Standarisasi"""

from sklearn.preprocessing import StandardScaler
 
numerical_features = ['Per_Sqft']
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""# MODELING

---
"""

# Siapkan dataframe untuk analisis model
models = pd.DataFrame(index=['train_mse', 'test_mse'], 
                      columns=['KNN', 'RandomForest', 'Boosting'])

"""### K-Nearest Neighbor"""

from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error
 
knn = KNeighborsRegressor(n_neighbors=10)
knn.fit(X_train, y_train)
 
models.loc['train_mse','knn'] = mean_squared_error(y_pred = knn.predict(X_train), y_true=y_train)

"""## Random Forest"""

# Impor library yang dibutuhkan
from sklearn.ensemble import RandomForestRegressor
 
# buat model prediksi
RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=55, n_jobs=-1)
RF.fit(X_train, y_train)
 
models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

"""### Boosting Algorithm"""

from sklearn.ensemble import AdaBoostRegressor
 
boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)                             
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

"""# EVALUATION

---

"""

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['KNN','RF','Boosting'])
 
# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'KNN': knn, 'RF': RF, 'Boosting': boosting}
 
# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3 
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3
 
# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)

"""### UJI DATA"""

prediksi = X_test.iloc[1:2].copy()
# prediksi = X_test.sample().copy()
pred_dict = {'y_true':y_test[1:2]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)
 
pd.DataFrame(pred_dict)